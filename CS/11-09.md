1. IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.
 - IPC란 프로세스간의 통신을 하기위한 방법을 뜻하며, 공유메모리(뮤텍스, 세마포어), 파일, 소켓, 명명 파이프, 메시지 큐가 있습니다.
2. Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.
 - Shared Memory란 여러 프로세스에서 동일한 메모리 블록에 대한 접근 권한이 부여되어 프로세스가 서로 통신할 수 있도록 공유버퍼를 생성하는 것을 말합니다. 동시에 여러 프로세스가 같은 메모리 블록에 접근하게 된다면 동기화 문제가 발생할 수 있기때문에 이를 컨트롤할 수 있는 세마포어나 뮤텍스와 기법이 필요할 수 있습니다.
3. 메시지 큐는 단방향이라고 할 수 있나요?
 - 메시지 큐는 기본적으로 단방향이지만, 송신용 큐와 수신용 큐를 각각 만들어 프로세스 간에 양방향 통신 구조를 구현할 수 있습니다.

4. Thread Safe 하다는 것은 어떤 의미인가요?
- Thread Safe란 여러 스레드가 동시에 같은 객체나 자원에 접근해도 잘못된 결과가 발생하지 않는 상태를 말합니다.
예를 들어 단순한 count++ 연산은 여러 스레드가 동시에 접근하면 값이 꼬일 수 있지만, AtomicInteger.incrementAndGet()을 사용하면 원자적으로 동작해 Thread Safe하게 동작합니다. 즉, 동시성 환경에서도 일관성과 무결성을 보장하는 걸 의미합니다.
5. Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요?
- Thread Safe를 보장하기 위해 가장 기본적으로는 synchronized 같은 락을 사용합니다.
하지만 락은 성능 오버헤드가 크기 때문에 상황에 따라 AtomicInteger처럼 CAS 기반의 원자 연산이나, 불변 객체 설계, ThreadLocal, ConcurrentHashMap 같은 자료구조를 활용할 수도 있습니다.
6. Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요.
- Peterson’s Algorithm은 두 스레드 간의 상호 배제를 보장하기 위한 소프트웨어 알고리즘입니다.
flag와 turn 변수를 이용해서 서로 임계 구역에 진입하지 않도록 양보하는 구조입니다.
다만 이 방법은 두 스레드만 적용 가능하고, Busy Waiting으로 CPU를 낭비하며, 현대 멀티코어 CPU에서는 캐시 일관성 문제로 제대로 동작하지 않을 수 있습니다.
그래서 실제 시스템에서는 하드웨어 수준의 Test-and-Set이나 Compare-And-Swap 같은 명령어로 락을 구현합니다.
7. Race Condition 이 무엇인가요?
- Race Condition은 여러 스레드가 동시에 같은 자원을 수정하거나 읽는 과정에서 실행 순서에 따라 결과가 달라지는 상황입니다.
예를 들어 count++ 연산이 세 단계(읽기 → 증가 → 쓰기)로 분리되어 있어서, 두 스레드가 동시에 실행하면 최종 결과가 2가 아니라 1이 될 수 있습니다.
이를 방지하기 위해 락이나 원자적 연산을 사용해서 동시 접근을 제어해야 합니다.
8. Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?
- 반드시 락을 써야 하는 건 아닙니다.
예를 들어 AtomicInteger나 AtomicReference는 내부적으로 Compare-And-Swap(CAS)을 사용해서 락 없이도 Thread Safe하게 동작합니다.
또, 불변 객체를 사용하면 상태 변경이 없으므로 Race Condition 자체가 발생하지 않습니다.
실무에서는 락 기반보다 이런 비차단(lock-free) 접근 방식을 선호하는 경우도 많습니다
